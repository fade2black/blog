{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my Perosnal Blog","text":"<p>This blog is a space where I share my journey with the Rust programming language. Here, you\u2019ll find my thoughts, experiments, and results as I explore Rust, cryptography, distributed systems, and cloud computing world. I\u2019ll also share useful code snippets and tips that I\u2019ve learned along the way. </p> <p>Find me on LinkedIn and Github.</p>"},{"location":"#archive","title":"Archive","text":""},{"location":"#secure-communication-with-kyber-and-dilithium-in-rust","title":"Secure Communication with Kyber and Dilithium in Rust","text":"<p> What Is Post-Quantum Cryptography? Quantum computers are no longer just science fiction\u2014they\u2019re getting real. And with them comes a big challenge: they could break many of the cryptographic systems we rely on today to keep our data safe. Most current encryption methods, like RSA or Elliptic Curve Cryptography,... </p> 21 Jul 2025"},{"location":"#implementing-distributed-cron-jobs-with-etcd","title":"Implementing Distributed Cron Jobs with etcd","text":"<p> Introduction In this article I\u2019ll walk you through a project I built using Rust that acts as a distributed cron scheduler. This project allows users to add, list, and remove cron jobs via a set of RESTful endpoints. The system continuously monitors the current time, ensuring that scheduled tasks ... </p> 07 Feb 2025"},{"location":"dcron/","title":"Distributed Cron","text":"16 Feb 2025"},{"location":"dcron/#implementing-distributed-cron-jobs-with-etcd","title":"Implementing Distributed Cron Jobs with etcd","text":""},{"location":"dcron/#introduction","title":"Introduction","text":"<p>In this article I\u2019ll walk you through a project I built using Rust that acts as a distributed cron scheduler. This project allows users to add, list, and remove cron jobs via a set of RESTful endpoints. The system continuously monitors the current time, ensuring that scheduled tasks are executed at the right moment. The advantage of being distributed is the increased availability of the service. If one node goes down, other nodes can continue processing requests and monitoring jobs.</p>"},{"location":"dcron/#the-inner-workings","title":"The Inner Workings","text":"<pre><code>flowchart\n    n1[\"etcd\"] &lt;--&gt; n2[\"Monitor\"] &amp; n3[\"http server\"]\n    n4[user] --&gt; n3[\"http server\"]\n    n1@{ shape: cyl}</code></pre> <p>The distributed cron starts by spawning a thread monitoring current time and http server for processing RESTful requests.</p> <pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    tracing_subscriber::fmt::init();\n\n    let result = tokio::try_join!(daemon::monitor(), run_http_server());\n\n    if let Err(err) = result {\n        error!(\"{err}\");\n    }\n\n    info!(\"Terminating...\");\n\n    Ok(())\n}\n</code></pre> <p>For processing endpoint requests, I\u2019ve used the Tokio Axum server. Cron jobs are stored in an etcd server as key-value pairs. When a new <code>add job</code> request is received, the application parses the submitted cron expression and command to execute. It then generates a unique key string and creates a JSON object with three fields to store in etcd: <code>pattern</code>, <code>next</code>, and <code>command</code>.</p> <ul> <li><code>Pattern</code>: holds the cron expression.</li> <li><code>Next</code>: stores the next scheduled execution time (the upcoming time when the command should run).</li> <li><code>Command</code>: holds the command that will be executed.</li> </ul> <p>This structure ensures that all cron jobs are stored consistently, and the use of the etcd server enables mutual access and synchronisation across multiple nodes.</p> <pre><code>async fn store_cron_job(json_str: &amp;str) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let mut client = Client::new().await?;\n\n    let lock_key = client.lock().await?;\n    let key = generate_unique_key(\"cron\");\n    client.store_cron_job(&amp;key, json_str).await?;\n    client.unlock(&amp;lock_key).await?;\n\n    Ok(())\n}\n</code></pre> <p>where <code>json_str</code> looks, for example, like</p> <pre><code>{\n  \"pattern\": \"* * * * *\",\n  \"next\": \"2025-02-16 14:14:00 +01:00\",\n  \"command\": \"echo hello, world\"\n}\n</code></pre> <p>When the server starts, it spawns a separate thread that periodically checks the current time. This thread locks access to the cron jobs, reads all the jobs from etcd, and processes them one by one. For each job, it checks the \"next\" occurrence time. If the scheduled time is less than the current time, the server spawns a separate child process to run the job\u2019s command, then updates the job's \"next\" occurrence time accordingly. After processing, the server sleeps for 3 seconds before repeating the process. </p> <pre><code>pub async fn monitor() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let mut client = Client::new().await?;\n\n    loop {\n        let lock_key = client.lock().await?;\n        let kvs = client.get_cron_jobs().await?;\n\n        for kv in kvs {\n            process(&amp;mut client, &amp;kv.0, &amp;kv.1).await?;\n        }\n        client.unlock(&amp;lock_key).await?;\n\n        time::sleep(time::Duration::from_secs(SLEEP_TIME)).await;\n    }\n}\n</code></pre> <p>The role of etcd in this application is to provide exclusive access to the cron jobs by offering a distributed locking mechanism. This is crucial because only one node should run a particular job at any given time. Both the monitoring thread and the HTTP actions that manipulate cron jobs stored in etcd lock a mutex before entering the critical section. Once the task is completed, the mutex is unlocked. For example,</p> <pre><code>pub async fn delete(key: &amp;str) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let mut client = Client::new().await?;\n\n    let lock_key = client.lock().await?;\n    client.delete_cron_job(key).await?;\n    client.unlock(&amp;lock_key).await?;\n\n    Ok(())\n}\n</code></pre>"},{"location":"dcron/#usage","title":"Usage","text":"<p>The easiest way to run the app is to build it and execute it locally. Assuming etcd is running locally, for example start three separate processes to emulate three different nodes. Be sure to set a unique port for each Axum server process and specify the <code>ETCD_URL</code> environment variable to match the etcd server URL.</p> <pre><code>export ETCD_URL=http:://1.2.3.4:1234\n./dcron\n</code></pre> <p>Another option is to create a Kubernetes deployment with three replicas of the server, along with a service that exposes a single IP address for interacting with the dcron server.</p>"},{"location":"dcron/#conclusion","title":"Conclusion","text":"<p>While this simple project is by no means a replacement for established frameworks like Kubernetes' CronJob, it served as an exploration of Rust's asynchronous features and the functionality of the etcd server. By building this distributed cron scheduler, I gained valuable hands-on experience with these tools and saw how they can be leveraged in real-world applications.</p>"},{"location":"kyber-dilithium/","title":"Kyber-Dilithium","text":"21 Jul 2025"},{"location":"kyber-dilithium/#secure-communication-with-kyber-and-dilithium-in-rust","title":"Secure Communication with Kyber and Dilithium in Rust","text":""},{"location":"kyber-dilithium/#what-is-post-quantum-cryptography","title":"What Is Post-Quantum Cryptography?","text":"<p>Quantum computers are no longer just science fiction\u2014they\u2019re getting real. And with them comes a big challenge: they could break many of the cryptographic systems we rely on today to keep our data safe.</p> <p>Most current encryption methods, like RSA or Elliptic Curve Cryptography, depend on math problems that are tough for regular (classical) computers to solve. But quantum computers could solve those same problems much faster using special algorithms like Shor\u2019s algorithm. That means the encrypted data we think is secure today might not be safe in the future.</p> <p>That\u2019s where post-quantum cryptography (PQC) comes in. It\u2019s a new kind of cryptography designed to resist attacks even from quantum computers. Instead of relying on things like factoring or discrete logarithms, PQC algorithms use different mathematical problems, like lattice-based constructions, that are believed to be hard for both classical and quantum computers to crack.</p> <p>Organizations like NIST (the National Institute of Standards and Technology) have been working on standardizing these new algorithms. Two of the most promising ones are Kyber (used for exchanging keys securely) and Dilithium (used for digital signatures). These are the tools I'll be using in my demo.</p> <p>It\u2019s also worth noting that NIST has selected an additional algorithm called Hamming Quasi-Cyclic (HQC) as a backup to Module-Lattice based Key Encapsulation Mechanism (ML-KEM) a.k.a Kyber, the primary choice for general-purpose encryption. HQC is based on different mathematical foundations, giving us more options if future research reveals weaknesses in lattice-based methods.</p>"},{"location":"kyber-dilithium/#digital-signatures-and-key-encapsulation-mechanisms-kems","title":"Digital Signatures and Key Encapsulation Mechanisms (KEMs)","text":"<p>Before I jump into my demo, it\u2019s good to understand two key building blocks: digital signatures and key encapsulation mechanisms (KEMs).</p> <p>A digital signature lets someone prove that a message (or data) really came from them and hasn\u2019t been tampered with. Think of it like signing a document\u2014except it's done with math. In my demo, the server uses the Dilithium signature scheme to sign its public key, so the client can verify it's talking to the right party and not an imposter.</p> <p>A key encapsulation mechanism (KEM) is a way for two parties to securely agree on a shared secret over an insecure network. One side publishes a public key, and the other uses it to generate a shared key and send it back in a \u201ccapsule\u201d (ciphertext). The owner of the private key can open the capsule and recover the same shared key. In my case, I use Kyber, a lattice-based KEM, to establish this shared secret.</p> <p>Now, you might be wondering: how is KEM different from regular public-key encryption?</p> <p>The main difference is that public-key encryption is typically used to encrypt arbitrary messages, while KEM is specifically designed to securely exchange secret keys. It\u2019s a streamlined, more efficient approach for key exchange\u2014especially useful when you want to switch to fast symmetric encryption (like AES) afterward, which is exactly what I do.</p>"},{"location":"kyber-dilithium/#communication-flow-using-kyber-dilithium","title":"Communication Flow Using Kyber + Dilithium","text":"<p>To show how post-quantum cryptography can be used in a real-world setting, I\u2019ve created a simple Rust demo that simulates a secure communication session between a client and a server.</p> <p>For the sake of the example, I pre-generated a Dilithium key pair and stored it in the project directory:</p> <ul> <li><code>keys/dil_pk.txt</code>: the public key (accessible to anyone)</li> <li><code>keys/dil_sk.txt</code>: the secret key (only accessible by the server)</li> </ul> <p>Let\u2019s walk through the communication process.</p> <p>Roles - Client (initiates the communication) - Server (responds and proves identity)</p> <p>Step 1: Client initiates communication - Client sends communication request (e.g. opens a TCP connection to the Server) - This is just the initial network setup \u2014 no cryptography yet.</p> <p>Step 2: Server sends its Kyber public key + Dilithium signature Server   - already has a pair of digital signature (dilithium) keys: <code>(pk_dil, sk_dil)</code>  - generates a pair of Kyber keys: <code>(pk_kyber, sk_kyber)</code>  - signs its Kyber public key using its Dilithium secret key: <code>signature = sign(pk_kyber, sk_dil)</code>  - sends the following payload to the client   <code>json  {     \"pk_kyber\": \"...\",     \"signature\": \"...\"  }</code> Step 3: Client verifies the signature  Client  - already has Server's Dilithium public key (<code>pk_dil</code>) through some trusted method (e.g., certificate, config, or manual distribution)  - verifies: <code>verify(pk_kyber, signature, pk_dil)</code>  - terminates the connection (because the client can\u2019t trust that the key is really from the server) if the client is unable to verify the signature, otherwise the client trusts the server. - generates and secret key and encapsulates it using Server's Kyber public key: <code>(ciphertext, shared_secret) = encapsulate(pk_kyber, rng)</code> - sends the <code>ciphertext</code> to the Server.</p> <p>Step 4: Server decpasulates the ciphertext Server - receives the ciphertext and uses its Kyber secret key to recover the shared secret: <code>shared_secret = decapsulate(ciphertext, sk_kyber)</code></p> <p>Now both Client and Server share the same secret.</p> <p>Step 5: Secure symmetric communication begins With the shared secret established, Server and Client switch to symmetric encryption (like AES-GCM) for fast, secure communication.</p>"},{"location":"kyber-dilithium/#demo-secure-communication-in-rust-using-kyber-dilithium-aes-gcm","title":"Demo: Secure Communication in Rust Using Kyber + Dilithium + AES-GCM","text":"<p>The full example is available in my GitHub repository, but here\u2019s the core part of the demo (the <code>main.rs</code> file) which ties everything together.</p> <p>This simple Rust program demonstrates: - how a client and server establish a shared secret using Kyber - how the server proves its identity using Dilithium signatures - and how both parties securely communicate using AES-GCM symmetric encryption</p> <pre><code>use kyber_playground::Server;\nuse kyber_playground::Client;\n\nfn main() {   \n    let mut client = Client::new(); \n    let client_messages = [\"Hi!\", \"What's up!\", \"Me also good!\"];\n    let mut server =  Server::new();\n    let server_messages = [\"Hi!\", \"All good! You?\", \"Glad to hear from you!\"];\n\n    // Client initiates a communication.\n    // Server accepts the connection and generates a (kyber public key, signature of the public key) pair. \n    let (pk, sig) = server.generate_pk_and_sig();\n\n    // Server sends the public key and signature to the client over a network.\n    // Client recieves the payload, verifies the public key, generates\n    // a secret key, encapsulates it, and sends it back to the server.\n    let ciphertext = client.accept_pk_and_sig(pk, sig);\n    // Server recieves the ciphertext and decapsulates it.\n    server.accept_ciphertext(ciphertext);\n    // At this stage both client and server have a shared secert key\n    // and now can securely communicate using one of secert key algorithms.\n    // e.g. AES-GCM\n    // Server and client start to communicate.\n    for i in 0..3 {\n        let plaintext = client_messages[i].as_bytes();\n        let (nonce, ciphertext) = client.encrypt_message(plaintext);\n        // Client sends the nonce and ciphertext over netwwork to the server \n        let decrypted = server.decrypt_message(&amp;ciphertext, nonce);\n        println!(\"Client: {:?}\", String::from_utf8(decrypted).unwrap());\n\n        let plaintext = server_messages[i].as_bytes();\n        let (nonce, ciphertext) = server.encrypt_message(plaintext);\n         // Server replies with the nonce and ciphertext over netwwork to the client\n        let decrypted = client.decrypt_message(&amp;ciphertext, nonce);\n        println!(\"Server: {:?}\", String::from_utf8(decrypted).unwrap());\n    }                \n}\n</code></pre> <p>This example demonstrates just the high-level flow. If you\u2019re curious about the implementation details\u2014like how keys are loaded from files, how Kyber and Dilithium are integrated, or how AES-GCM is used under the hood\u2014feel free to explore the full source code on GitHub.</p> <p>In the next part, I\u2019m planning to expand the demo into a more realistic setup, with separate client and server processes communicating over a network using the TCP protocol. This version will include key exchange and secure message transmission using symmetric encryption (AES-GCM) in a real network environment.</p>"},{"location":"leader-election/","title":"Leader Election","text":"07 Feb 2025"},{"location":"leader-election/#building-leader-election-in-distributed-systems-with-etcd","title":"Building Leader Election in Distributed Systems with etcd","text":""},{"location":"leader-election/#introduction","title":"Introduction","text":"<p>In distributed systems, leader election is a critical process that ensures coordination and consistency across multiple nodes. One way to implement leader election is through the use of a reliable key-value store, such as etcd. In this article, I'll explore how to implement a simple leader election implementation in Rust using an etcd server. Briefly, the 'leader election' problem in a distributed system addresses the coordination challenge among multiple nodes. The goal is to elect a leader node that makes decisions on behalf of all nodes in a cluster, avoiding conflicts, race conditions, and inconsistencies. Etcd is a distributed key-value store providing a reliable way to store data across a cluster of nodes (machines), ensuring consistency and availability. Besides the basic CRUD operations on key-value pairs, etcd provdes other crucial components such lease and distributed shared lock. In simple terms, a lease is a mechanism used to manage the lifetime of a key-value pair in the etcd key-value store. You can create a lease with a specific Time-To-Live (TTL) and then associate it with a key. When the lease\u2019s TTL expires, the key is automatically deleted. Similarly, distributed lock is used to coordinate access to a shared resource in a distributed system, ensuring that only one node can hold the lock at a time. These features, when combined, form the core of a leader election service. Version 3 of etcd introduces a leader election provides along with the corresponding methods.</p>"},{"location":"leader-election/#implementation","title":"Implementation","text":"<p>The solution is based on a lease acquisition and renewal mechanism. When a node starts, it spawns a separate task that creates a lease and then calls the campaign method with the lease ID as a parameter. If multiple nodes are competing to be elected as the leader, only one node is selected, while the others are blocked by the campaign method. The blocked nodes remain in this state until the current leader either relinquishes leadership or fails, e.g., due to network partitioning at which point the lease expires. Once the lease expires, the blocked nodes are awakened and will retry to acquire leadership. Once a node acquires leadership, it maintains its status by periodically sending keep-alive requests to the etcd server to renew the lease. This ensures that the lease doesn't expire while the other nodes remain blocked and wait for the next leader election phase.</p> <p><pre><code>async fn participate_in_election(args: &amp;Args) -&gt; Result&lt;(), Error&gt; {\n    let mut client = connect(args).await?;\n\n    loop {\n        let resp = client.lease_grant(TTL, None).await?;\n        let lease_id = resp.id();\n\n        info!(\"Starting a new campaign.\");\n        let resp = client\n            .campaign(ELECTION_NAME, args.node.clone(), lease_id)\n            .await?;\n        let leader_key = resp\n            .leader()\n            .ok_or(Error::ElectError(\"Failed to retrieve the leader.\".into()))?;\n        info!(\"\ud83e\udd73 I am the leader ({})\", args.node);\n\n        if let Ok((mut keeper, _)) = client.lease_keep_alive(lease_id).await {\n            loop {\n                info!(\"\u23f0 Keeping alive the lease {}...\", leader_key.key_str()?);\n                keeper.keep_alive().await?;\n                time::sleep(Duration::from_secs(7)).await;\n            }\n        } else {\n            error!(\"Failed to keep lease alive. Re-campaigning.\");\n        }\n    }\n}\n</code></pre> At the same time, nodes observe election proclamations in order, as made by the elected leaders, to stay aware of the current leader.</p> <pre><code>async fn observe_election(args: &amp;Args, state: Arc&lt;RwLock&lt;State&gt;&gt;) -&gt; Result&lt;(), Error&gt; {\n    let mut client = connect(args).await?;\n\n    let mut msg = client.observe(ELECTION_NAME).await?;\n    loop {\n        if let Some(resp) = msg.message().await? {\n            let kv = resp\n                .kv()\n                .ok_or(Error::WatchError(\"Unable to retrieve key/value\".into()))?;\n            let key = kv.key_str()?;\n            let val = kv.value_str()?;\n\n            let mut st = state.write().await;\n            (*st).is_leader = val == args.node;\n            info!(\n                \"\ud83d\udfe2 Current leader is {val} with key {key}, node.is_leader={}\",\n                (*st).is_leader\n            );\n        }\n    }\n}\n</code></pre> <p>You can find the full implementation here</p> <p>To launch the entire process for three nodes, you can start each node in a separate console window as follows</p> <pre><code>% node --node node1 --host 127.0.0.1 --port 50686\n</code></pre> <pre><code>% node --node node2 --host 127.0.0.1 --port 50686\n</code></pre> <pre><code>% node --node node3 --host 127.0.0.1 --port 50686\n</code></pre> <p>where the <code>host</code> and <code>port</code> refer to the URL and port number of the etcd server used to interact with the etcd cluster.</p>"},{"location":"leader-election/#conclusion","title":"Conclusion","text":"<p>eader election is a critical component in distributed systems, ensuring high availability and fault tolerance. The implementation discussed here serves as a starting point, but to build a production-ready leader election system, it\u2019s important to consider additional nuances specific to your use case. For example, factors like network partitioning, node failure handling, and consistency guarantees should be carefully addressed. For a deeper dive into best practices, AWS provides insights and best-practice recommendations.</p>"}]}